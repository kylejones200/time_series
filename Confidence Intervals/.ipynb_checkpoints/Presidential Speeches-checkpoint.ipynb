{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52be2240-0b5b-4cb0-91fa-d12702b17c6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'speech_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Apply text cleaning\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Define keywords to track\u001b[39;00m\n\u001b[1;32m     54\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreedom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgovernment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'speech_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Define the URL of the archive\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/inaugural-addresses\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract links to speeches\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "speech_links = [base_url + a[\"href\"] for a in soup.find_all(\"a\", href=True) if \"documents/inaugural-address\" in a[\"href\"]]\n",
    "\n",
    "# Function to scrape speech text and year\n",
    "def fetch_speech_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join(p.text for p in paragraphs)\n",
    "    \n",
    "    # Extract the year from the title\n",
    "    title = soup.find(\"title\").text\n",
    "    year_match = re.search(r\"(\\d{4})\", title)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "    \n",
    "    return year, text\n",
    "\n",
    "# Scrape all speeches\n",
    "data = []\n",
    "for link in speech_links[:10]:  # Limit for testing\n",
    "    year, text = fetch_speech_text(link)\n",
    "    if year and text:\n",
    "        data.append({\"year\": year, \"speech_text\": text})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Remove line breaks\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['clean_text'] = df['speech_text'].apply(clean_text)\n",
    "\n",
    "# Define keywords to track\n",
    "keywords = [\"freedom\", \"economy\", \"war\", \"rights\", \"government\"]\n",
    "\n",
    "# Count occurrences per speech\n",
    "for word in keywords:\n",
    "    df[word] = df['clean_text'].apply(lambda x: x.count(word))\n",
    "\n",
    "# Aggregate by year\n",
    "df_grouped = df.groupby('year')[keywords].sum()\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "for word in keywords:\n",
    "    plt.plot(df_grouped.index, df_grouped[word], label=word)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Word Frequency\")\n",
    "plt.title(\"Political Themes in U.S. Inaugural Addresses (1789-Present)\")\n",
    "plt.legend()\n",
    "plt.savefig(\"political_speech_trends.png\")\n",
    "plt.show()\n",
    "\n",
    "# Convert text data into word frequency matrix\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Apply LDA topic modeling\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topics = lda.fit_transform(X)\n",
    "\n",
    "# Print top words per topic\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}: \", [words[i] for i in topic.argsort()[:-10 - 1:-1]])\n",
    "\n",
    "# Assign dominant topic\n",
    "df['dominant_topic'] = topics.argmax(axis=1)\n",
    "\n",
    "# Aggregate topic distributions by year\n",
    "topic_trends = df.groupby('year')['dominant_topic'].value_counts().unstack()\n",
    "\n",
    "# Plot the evolution of themes\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_trends.plot(kind=\"area\", stacked=True)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Proportion of Speeches\")\n",
    "plt.title(\"Thematic Evolution in Inaugural Addresses\")\n",
    "plt.savefig(\"speech_topic_trends.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d1cb3e-f165-41b6-8143-40b0cfcd0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "RangeIndex(start=0, stop=0, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # See if 'speech_text' exists\n",
    "print(df.columns)  # Check available columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858b70e5-4046-4483-915f-20f8ab4fb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_speech_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \" \".join(p.text.strip() for p in paragraphs if p.text.strip())  # Ensure non-empty text\n",
    "    \n",
    "    title = soup.find(\"title\").text\n",
    "    year_match = re.search(r\"(\\d{4})\", title)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "    return year, text if text else None  # Return None if empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0877cd-3f58-492e-b5cb-ebba6f8a1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if \"documents/inaugural-address\" in a[\"href\"]]\n",
    "speech_links = list(set(speech_links))  # Remove duplicates\n",
    "speech_links = [base_url + link if not link.startswith(\"http\") else link for link in speech_links]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc41c229-49fa-4bbd-a156-1101ec56f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_speech_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # Extract speech paragraphs\n",
    "    paragraphs = soup.select(\"div.field-docs-content p\")  # Adjust based on the webpage structure\n",
    "    text = \" \".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "    \n",
    "    # Extract the year from the title\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    year_match = re.search(r\"(\\d{4})\", title)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "    return year, text if text else None  # Return None if empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f92430-9bbd-4d28-b632-134e8348c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for link in speech_links[:10]:  \n",
    "    year, text = fetch_speech_text(link)\n",
    "    if year and text:\n",
    "        data.append({\"year\": year, \"speech_text\": text})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())  # Should show actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "736a4af0-1ffc-4ff3-a5b2-8605d68189e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speech links found: 0\n",
      "First 5 links: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/inaugural-addresses\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract links\n",
    "speech_links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if \"documents/inaugural-address\" in a[\"href\"]]\n",
    "speech_links = list(set(speech_links))  # Remove duplicates\n",
    "speech_links = [\"https://www.presidency.ucsb.edu\" + link if not link.startswith(\"http\") else link for link in speech_links]\n",
    "\n",
    "print(\"Number of speech links found:\", len(speech_links))\n",
    "print(\"First 5 links:\", speech_links[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7dfa131-b762-4689-abf0-3c5388120377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\n",
      "  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html dir=\"ltr\" lang=\"en\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <script type=\"text/javascript\">\n",
      "   (window.NREUM||(NREUM={})).init={ajax:{deny_list:[\"bam.nr-data.net\"]}};(window.NREUM||(NREUM={})).loader_config={licenseKey:\"dee899de70\",applicationID:\"80106271\"};;/*! For license information please see nr-loader-rum-1.280.0.min.js.LICENSE.txt */\n",
      "(()=>{var e,t,r={122:(e,t,r)=>{\"use strict\";r.d(t,{a:()=>i});var n=r(944);function i(e,t){try{if(!e||\"object\"!=typeof e)return(0,n.R)(3);if(!t||\"object\"!=typeof t)return(0,n.R)(4);const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(r).length?e:r;for(let a in o)if(void 0!==e[a])try{if(null===e[a]){r[a]=null;continue}Array.isArray(e[a])&&Array.isArray(t[a])?r[a]=Array.from(new Set([...e[a],...t[a]])):\"object\"==typeof e[a]&&\"object\"==typeof t[a]?r[a]=i(e[a],t[a]):r[a]=e[a]}catch(e){(0,n.R)(1,e)}return r}catch(e){(0,n.R)(2,e)}}},555:(e,t,r)=>{\"use strict\";r.d(t,{Vp:()=>c,fn:()=>s,x1:()=>u});var n=r(384),i=r(122);const o={beacon:n.NT.beacon,errorBeacon:n.NT.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},a={};function s(e){try{const t=c(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}function c(e){if(!e)throw new Error(\"All info objects require an agent identifier!\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/inaugural-addresses\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# View raw HTML\n",
    "print(soup.prettify()[:2000])  # Print first 2000 characters to inspect the page structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f781587-77b1-461a-9cea-83cc911b836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 404\n",
      "Response Headers: {'Connection': 'keep-alive', 'Cache-Control': 'no-cache, must-revalidate', 'Content-Encoding': 'gzip', 'Content-Language': 'en', 'Content-Type': 'text/html; charset=utf-8', 'Expires': 'Sun, 19 Nov 1978 05:00:00 GMT', 'Link': '</oops>; rel=\"canonical\",</node/324522>; rel=\"shortlink\"', 'Server': 'nginx', 'Strict-Transport-Security': 'max-age=300', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Generator': 'Drupal 7 (http://drupal.org)', 'X-Pantheon-Styx-Hostname': 'styx-fe1-b-6f847bcb88-s5tjv', 'X-Styx-Req-Id': '411bab02-e18b-11ef-a8e8-5ee6e89791af', 'Date': 'Sun, 02 Feb 2025 17:29:28 GMT', 'X-Served-By': 'cache-chi-kigq8000094-CHI, cache-mci680044-MCI', 'X-Cache': 'MISS, MISS', 'X-Cache-Hits': '0, 0', 'X-Timer': 'S1738517368.054851,VS0,VE155', 'Vary': 'Accept-Encoding, Cookie, Cookie', 'Age': '0', 'Accept-Ranges': 'bytes', 'Via': '1.1 varnish, 1.1 varnish', 'transfer-encoding': 'chunked'}\n",
      "First 500 characters of response: <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\n",
      "  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      "<head profile=\"http://www.w3.or\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/inaugural-addresses\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Headers:\", response.headers)\n",
    "print(\"First 500 characters of response:\", response.text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "017f7c80-2a92-4983-9667-d8b164614e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 inaugural addresses.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Updated URL\n",
    "base_url = \"https://www.presidency.ucsb.edu\"\n",
    "url = \"https://www.presidency.ucsb.edu/documents/app-categories/spoken-addresses-and-remarks/presidential/inaugural-addresses\"\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract links to speeches\n",
    "speech_links = []\n",
    "for a in soup.select(\"td.views-field-title a\"):  # Based on observed structure\n",
    "    href = a.get(\"href\")\n",
    "    if href and \"documents\" in href:\n",
    "        speech_links.append(base_url + href)\n",
    "\n",
    "print(f\"Found {len(speech_links)} inaugural addresses.\")\n",
    "\n",
    "# Function to extract speech text and year\n",
    "def fetch_speech_text(url):\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Extract speech text (adjust based on actual structure)\n",
    "    paragraphs = soup.select(\"div.field-docs-content p\")\n",
    "    text = \" \".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "\n",
    "    # Extract the year from the title\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    year_match = re.search(r\"(\\d{4})\", title)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "    return year, text\n",
    "\n",
    "# Scrape all speeches\n",
    "data = []\n",
    "for link in speech_links[:10]:  # Limiting to 10 for testing\n",
    "    year, text = fetch_speech_text(link)\n",
    "    if year and text:\n",
    "        data.append({\"year\": year, \"speech_text\": text})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())  # Verify content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01b7a0-46e7-44c3-870d-16c4baeafd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
